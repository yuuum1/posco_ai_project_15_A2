{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#환경설정\" data-toc-modified-id=\"환경설정-0.1\">환경설정</a></span></li></ul></li><li><span><a href=\"#Google-Cloud-STT-API\" data-toc-modified-id=\"Google-Cloud-STT-API-1\">Google Cloud STT API</a></span><ul class=\"toc-item\"><li><span><a href=\"#(1분-이하)-동기-음성-인식\" data-toc-modified-id=\"(1분-이하)-동기-음성-인식-1.1\">(1분 이하) 동기 음성 인식</a></span></li><li><span><a href=\"#(1분-이상)-비동기-음성-인식\" data-toc-modified-id=\"(1분-이상)-비동기-음성-인식-1.2\">(1분 이상) 비동기 음성 인식</a></span></li></ul></li><li><span><a href=\"#STT-하고-후처리(?)\" data-toc-modified-id=\"STT-하고-후처리(?)-2\">STT 하고 후처리(?)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#띄어쓰기\" data-toc-modified-id=\"띄어쓰기-2.0.1\">띄어쓰기</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workstation conda env. python 3.8에서 구동\n",
    "# m1 mac miniforge env. python 3.8에서도 구동 가능\n",
    "\n",
    "# !pip install keras kss jupyter sklearn numpy wave contextlib2 SpeechRecognition plotly regex matplotlib tensorflow-gpu pydub\n",
    "# !pip install google-cloud-storage google-cloud-speech google-cloud-datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T05:37:11.197778Z",
     "start_time": "2021-09-28T05:36:49.297367Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Kss...\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "import os\n",
    "import io\n",
    "import argparse\n",
    "from os import path\n",
    "\n",
    "# API credential 설정\n",
    "    # 이거는 내 계정 credential이기 때문에.. 너무 많이 실행하면.. ^^\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"singular-object.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T05:37:11.261313Z",
     "start_time": "2021-09-28T05:37:11.205180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihun/.pyenv/versions/3.8.8/envs/nlp/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='processed_audio2.wav'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### audio preprocessing: 1분미만파일(custom.wav) ###\n",
    "# sampling rate => 16000, stereo => mono 로 맞춰주기\n",
    "from pydub import AudioSegment\n",
    "sound = AudioSegment.from_wav('custom2.wav')\n",
    "sound = sound.set_channels(1)\n",
    "sound = sound.set_frame_rate(16000)\n",
    "\n",
    "sound.export('processed_audio2.wav', format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud STT API\n",
    "\n",
    "- 계정 만들면 1년? 동안 무료 크레딧 35만원 정도 쓸 수 있게 해줌\n",
    "    - 내 계정은 12월까지 크레딧 사용 가능\n",
    "- STT API 호출 시 첫 60분(음성파일 길이 기준)은 무료\n",
    "- 60분 넘어가면 그때부터 15초당 0.006달러 청구됌. (청구되도 무료 크레딧 있어서 돈은 안나갈듯)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1분 이하) 동기 음성 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T05:37:11.272837Z",
     "start_time": "2021-09-28T05:37:11.264643Z"
    }
   },
   "outputs": [],
   "source": [
    "# wav file 사용\n",
    "def transcribe_file(speech_file):\n",
    "    \n",
    "    # wav 길이 구하기\n",
    "    sec = 0\n",
    "    with contextlib.closing(wave.open('processed_audio2.wav', 'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "        sec += duration\n",
    "     \n",
    "    # clinet 설정\n",
    "    from google.cloud import speech\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # 오디오 파일 열기\n",
    "    with io.open(speech_file, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "    \n",
    "    # config 설정\n",
    "    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(encoding=encoding,\n",
    "                                      sample_rate_hertz=16000,\n",
    "                                      language_code='ko-KR')\n",
    "    \n",
    "    # response 받기\n",
    "    resp = client.recognize(config=config, audio=audio)\n",
    "    \n",
    "    s_data = ''\n",
    "    conf = ''\n",
    "    for result in resp.results:\n",
    "        s_data = result.alternatives[0].transcript\n",
    "        conf= result.alternatives[0].confidence\n",
    "    \n",
    "    return s_data, conf, sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T05:38:33.140384Z",
     "start_time": "2021-09-28T05:37:11.275201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_dat0a, conf0, sec0 = transcribe_file('processed_audio2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T05:38:33.156855Z",
     "start_time": "2021-09-28T05:38:33.147624Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('취업준비생에게 면접이란 수년간의 노력을 짧은 시간에 평가받는 중요한 순간이지만 충분히 연습하지 못해 면접을 망치는 경우가 많습니다 사람이네 조사했다면 면접 경험 있는 취준생들이 뽑은 면접 성공을 위해 중요한 준비 중 면접 표정이 연습이 40%로 매우 중요한 준비물 알 수 있습니다 그러나 코로나 19 이후 여러 명이 모여 하던 모의면접 연습이 어려워짐에 따라서 취업 준비 개인이 모의면접 연습을 통해 객관적인 피드백을 받기 어려운 상황입니다',\n",
       " 0.9211312532424927,\n",
       " 41.792)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_dat0a, conf0, sec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1분 이상) 비동기 음성 인식\n",
    "- 비동기 음성 인식 하려면 로컬에 있는 음성 파일을 '구글 클라우드 스토리지 버킷'에 저장해야 한다.\n",
    "    - 스토리지 버킷은 생성하면 1기가당 월 0.023 달러 청구됌.\n",
    "    - 일단 10기가 생성했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:20:29.825103Z",
     "start_time": "2021-09-26T05:20:29.627301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='processed_audio1.wav'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### audio preprocessing: 3분파일(custom1.wav) ###\n",
    "# sampling rate => 16000, stereo => mono 로 맞춰주기\n",
    "from pydub import AudioSegment\n",
    "sound = AudioSegment.from_wav('custom1.wav')\n",
    "sound = sound.set_channels(1)\n",
    "sound = sound.set_frame_rate(16000)\n",
    "\n",
    "sound.export('processed_audio1.wav', format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:20:31.339620Z",
     "start_time": "2021-09-26T05:20:31.333504Z"
    }
   },
   "outputs": [],
   "source": [
    "# cloud bucket에 올리기\n",
    "from google.cloud import storage\n",
    "import glob\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "        source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:20:32.772948Z",
     "start_time": "2021-09-26T05:20:31.753249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed_audio1.wav uploaded to audio_test/processed_audio1.wav.\n"
     ]
    }
   ],
   "source": [
    "# wav 파일 버킷에 업로드하기\n",
    "upload_blob('posco_15a2', 'processed_audio1.wav', 'audio_test/processed_audio1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:20:33.029717Z",
     "start_time": "2021-09-26T05:20:33.003604Z"
    }
   },
   "outputs": [],
   "source": [
    "# 버킷 uri 가져오기 (stt api가 uri를 파라미터로 받음)\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'posco_15a2'\n",
    "blob_name = '/audio_test/processed_audio1.wav'\n",
    "gcs_uri = 'gs://' + bucket_name + blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:20:34.570100Z",
     "start_time": "2021-09-26T05:20:34.553689Z"
    }
   },
   "outputs": [],
   "source": [
    "### bucket에 있는 파일 비동기 음성 인식 ###\n",
    "def transcribe_gcs(gcs_uri, filename):\n",
    "    \n",
    "    ### bucket에 있는 파일을 불러와서 sec 구하기 ###\n",
    "    # 버킷 url 가져오기\n",
    "    bucket = 'posco_15a2'\n",
    "    gcs_url = 'https://%(bucket)s.storage.googleapis.com/%(file)s' % {'bucket':bucket, 'file':filename}\n",
    "\n",
    "    # url로 wav 파일 가져오기\n",
    "    import urllib\n",
    "    import ssl\n",
    "    context = ssl._create_unverified_context()\n",
    "    audio_file = urllib.request.urlopen(gcs_url, context=context)\n",
    "\n",
    "    # sec 구하기\n",
    "    sec = 0\n",
    "    with contextlib.closing(wave.open(audio_file, 'r')) as f:\n",
    "        frames = f.getnframes()\n",
    "        rate = f.getframerate()\n",
    "        duration = frames / float(rate)\n",
    "        sec += duration\n",
    "\n",
    "    \n",
    "    ### 비동기 음성 인식 ###\n",
    "    # client 선언\n",
    "    from google.cloud import speech\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    # config 설정\n",
    "    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    config = speech.RecognitionConfig(encoding=encoding,\n",
    "                                      sample_rate_hertz=16000,\n",
    "                                      language_code='ko-KR')\n",
    "    \n",
    "    # 음성인식 실행\n",
    "    operation = client.long_running_recognize(config=config,\n",
    "                                              audio=audio)\n",
    "    \n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    response = operation.result(timeout=90)\n",
    "    \n",
    "    # 음성파일이 긴 경우 response에서 transcript와 confidence가 여러 array로 구분되어서 반환됌.\n",
    "    # => 그래서 transcript는 모두 이어주고, confidence는 모두 더해서 array 숫자로 나눠줌\n",
    "    s_data = ''\n",
    "    conf = 0\n",
    "    num = 0\n",
    "    for i in response.results:\n",
    "        s_data += i.alternatives[0].transcript\n",
    "        conf += i.alternatives[0].confidence\n",
    "        num += 1\n",
    "    conf = conf / num\n",
    "    \n",
    "    return s_data, conf, sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-26T05:19:56.219863Z",
     "start_time": "2021-09-26T05:19:29.409931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete...\n"
     ]
    }
   ],
   "source": [
    "s_data, conf, sec = transcribe_gcs(gcs_uri, 'audio_test/processed_audio1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-25T15:21:54.443641Z",
     "start_time": "2021-09-25T15:21:54.434460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('안녕하십니까 아 이건 발표를 맡게 된 A3 조신 정우입니다 저해제는 발표연습 도우미인 프레젠테이션입니다 목차는 다음과 같습니다 저희 프로젝트 추진 배경입니다 spt 상황이 많이 많아진 자기표현 시대의 발표 능력 향상을 목표로 하는 소비자의 니즈가 증가하고 있습니다 그리고 발표를 두려워하거나 능력이 부족한 사람들을 위한 서비스가 부재 하기 때문에 발표 연습을 도와주는 AI 프로젝트를 계획했습니다 현재 기술현황으로는 마이크로소프트에서 제공하는 프레젠테이션 코치와 면접연습 aren 뷰인터가 있습니다 프레젠테이션 고치의 경우 필러 유무 서비스 등 장점이 있지만 파워포인트에서만 이용이 가능하고 사용이 가능하다는 단점이 있습니다 그래서 프레젠테이션에서는 로션 프레지 등 다양한 발표 도구를 사용할 수 있으며 한국어 발표 연습이 가능하도록 목표를 설정했습니다뷰인터의 경우 영상을 통해 시각과 음성을 분석하는 서비스를 제공하는데이 기술을 참고하여 시선처리와 표정으로 시각을 분석하고 대본과 실제 발표 음성을 비교하여 분석하는 것을 목표로 설정했습니다 폰에 젠테이션 회의시스템 구조는 발표 대본을 먼저 업로드하고 발표 영상을 녹화하면 게이지 트레킹과 감정 감정 인식으로 시각 분석을 하고 실제 발표를 stt를 이용하여 텍스트와 시켜서 워드투벡터로 업로드하는 대본과 비교영상 문자를 합니다 그리고 STP에서 나오는 단어수를 파악해 말해 속도 자주 사용하는 단어 등을 결과레포트 에그 나타날것입니다 발표가 끝나면 이런 결과들을 보고서에 증명하는 발표 연습을 끝내다거나 만족하지 못할 경우 대본을 제 업로드하거나 발표 연습을 다시 하는 구조입니다 다음으로는 사용하는 기술을 소개하겠습니다 데이즈 트레킹정우 사용자의 시선이 화면에서 극단적으로 벗어나는 경우를 의심 수준으로 설정하고 그 횟수를 봐 갈 것입니다 이모션 테크니션의 경우 발표자의 효정중 앵그리 내추럴 해피를 인식하고 그 결과를 시계를 그래프로 나타내 결과 보고서에서 출력할 것입니다 st는 사용자의 말하기를 문자 바꿔서 실제 발표내용을 확인하여 발표능력 피드백에 활용 할 것입니다 1루트 100은 코엘 파일을 사용하여 업로드란 대본과를 이용하여 만든 텍스트를 비교하여 단어별로 유사도를 체크 할 것입니다 먼저 시각분석 진행 상황입니다 사진에서 빨간 네모박스를 보시면 이모션 코그니션을 하는 것으로 표정을 인식하여 현재 사진에는 미추리 나타내고 있습니다눈동자에는 초록색 십자가 표시가 있는데 데이즈 트레킹을 하는 것입니다',\n",
       " 0.8869973868131638,\n",
       " 181.54)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_data, conf, sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T11:41:43.250690Z",
     "start_time": "2021-09-27T11:41:43.215165Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hh/14z5pty92_s51gdc2n406w6w0000gn/T/ipykernel_84446/2612060835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's_data' is not defined"
     ]
    }
   ],
   "source": [
    "s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT 하고 후처리(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 띄어쓰기\n",
    "> google cloud api는 띄어쓰기 된 상태로 return 한다.\n",
    "- 11기는 띄어쓰기 하기 위해 KoPySpacing 라이브러리 썼는데, 이 라이브러리가 설치가 잘 안 된다. (텐서플로 버전 문제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마침표 붙이기\n",
    "def put(s_data):\n",
    "    doc = \"\"\n",
    "    # 마침표 붙이기\n",
    "    for sent in kss.split_sentences(s_data):\n",
    "        doc += sent + '..'   # 왜 마침표 두 개 붙여놨지?\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put(s_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속도 시각화\n",
    "def check_speed(s_data, sec):\n",
    "    syllable = ''.join(s_data.split())  # 공백 제외 글자 수(음절 수)\n",
    "\n",
    "    # 평균 발표 속도: 250 / min\n",
    "    # 발표 속도: 음절개수 / min\n",
    "    speaking_rate = len(syllable) * 60 / sec\n",
    "\n",
    "    \n",
    "#### go install 안됌\n",
    "\n",
    "#     fig = go.Figure(go.Indicator(\n",
    "#         domain={'x': [0, 1], 'y': [0, 1]},\n",
    "#         value=speaking_rate,\n",
    "#         mode=\"gauge+number+delta\",\n",
    "#         title={'text': \"Speed\"},\n",
    "#         delta={'reference': 250},\n",
    "#         gauge={'axis': {'range': [None, 500]},\n",
    "#                'steps': [\n",
    "#                    {'range': [0, 250], 'color': \"lightgray\"},\n",
    "#                    {'range': [250, 500], 'color': \"gray\"}],\n",
    "#                'threshold': {'line': {'color': \"red\", 'width': 4}, 'thickness': 0.75, 'value': 490}}))\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "    return speaking_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_speed(s_data, sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필러 체크\n",
    "def check_filler(doc):\n",
    "    filler = ['뭐', '음', '그', '어', '그냥', '이제', '좀', '아', '한',\n",
    "              '그거', '대게', '막', '그게', '그니까', '그래', '근데',\n",
    "              '일단', '아마', '저기', '이', '뭐지', '뭔가', '스', '하', '자',\n",
    "              '에', '이게', '뭐더라']\n",
    "    # filler 제거 check\n",
    "    remove_doc = []\n",
    "\n",
    "    list_filler = doc.split()\n",
    "\n",
    "    s_filler = dict()\n",
    "\n",
    "    for i in range(len(list_filler)):\n",
    "        if list_filler[i] in filler:\n",
    "            remove_doc.append(i)\n",
    "            if s_filler.get(list_filler[i]):\n",
    "                s_filler[list_filler[i]] += 1\n",
    "            else:\n",
    "                s_filler[list_filler[i]] = 1\n",
    "    # filler 제거한 doc 문장\n",
    "    list(map(list_filler.pop, remove_doc))\n",
    "    m_doc = ''.join(list_filler)\n",
    "    return m_doc, s_filler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_filler(s_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(s_data)\n",
    "sent_tokens = sent_tokenize(put_s_data) # 문단별 토크나이징은 '마침표' 기준인듯\n",
    "\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scan_vocabulary(sents, tokenize, min_counts=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w: c for w, c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(),\n",
    "                                         key: lambda x:-x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
