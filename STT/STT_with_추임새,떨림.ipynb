{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STT_final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "LB9EPju-IFBH"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6QlO8uFvwWm",
        "outputId": "fef284da-5c15-4efc-e679-9da04274a7d7"
      },
      "source": [
        "!pip install pydub\n",
        "!pip install SpeechRecognition\n",
        "!pip install praat-parselmouth"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 82 kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n",
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.0-cp37-cp37m-manylinux2010_x86_64.whl (10.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.7 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from praat-parselmouth) (1.19.5)\n",
            "Installing collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD9pBs7ev-Yx"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_silence\n",
        "from pydub.silence import detect_nonsilent\n",
        "from keras.models import load_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import sklearn\n",
        "import speech_recognition as sr\n",
        "import json\n",
        "\n",
        "import parselmouth\n",
        "import glob\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Audio"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrfTmm39wBT9",
        "outputId": "56081bec-3705-4574-e8ad-0d4b22634bd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caQpYdG0rGCt"
      },
      "source": [
        "## 비추임새판별 / 추임새분류 Model 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MhBOshwCc1"
      },
      "source": [
        "filler_determine_model = load_model('/content/drive/MyDrive/Colab Notebooks/posco_ai_project/SpeakUP_ML/isfiller_classifier_0928.h5')\n",
        "filler_classifier_model = load_model('/content/drive/MyDrive/Colab Notebooks/posco_ai_project/SpeakUP_ML/filler_classifier_0928.h5')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNq__Z6_rVSa"
      },
      "source": [
        "## 몇몇 기능 정의\n",
        "- mfcc padding\n",
        "- scaler\n",
        "- 음성파일 amplitude 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMs0AbDzwVP5"
      },
      "source": [
        "pad1d = lambda a, i: a[0: i] if a.shape[0] > i else np.hstack((a, np.zeros(i-a.shape[0])))\n",
        "pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "#adjust target amplitude\n",
        "def match_target_amplitude(sound, target_dBFS):\n",
        "    change_in_dBFS = target_dBFS - sound.dBFS\n",
        "    return sound.apply_gain(change_in_dBFS)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIPWgsburkRl"
      },
      "source": [
        "## 비추임새판별 함수 / 추임새 분류 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5-Xjt_Kwh01"
      },
      "source": [
        "def predict_filler(audio_file):\n",
        "    # 추임새 판별을 위한 임시 음성 파일 생성\n",
        "    audio_file.export(\"temp.wav\", format=\"wav\")\n",
        "\n",
        "    wav, sr = librosa.load(\"temp.wav\", sr=16000)\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(wav, sr=16000, n_mfcc=100, n_fft=400, hop_length=160)\n",
        "    #mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
        "    mfcc = scaler.fit_transform(mfcc)\n",
        "\n",
        "    padded_mfcc = pad2d(mfcc, 40)\n",
        "    padded_mfcc = np.expand_dims(padded_mfcc, [0, 3])  # (100,40) 에서 (1,100,40,1) 으로 차원 추가\n",
        "\n",
        "    result = filler_determine_model.predict(padded_mfcc)\n",
        "\n",
        "    # 판별 완료된 음성 파일 삭제\n",
        "    os.remove(\"temp.wav\")\n",
        "\n",
        "    if result[0][0] >= result[0][1]: # 추임새\n",
        "        return 0 \n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK9xFxBZxPSw"
      },
      "source": [
        "def predict_filler_type(audio_file):\n",
        "    # 추임새 분류를 위한 임시 음성 파일 생성\n",
        "    audio_file.export(\"temp.wav\", format=\"wav\")\n",
        "\n",
        "    wav, sr = librosa.load(\"temp.wav\", sr=16000)\n",
        "    mfcc = librosa.feature.mfcc(wav, sr=16000, n_mfcc=100, n_fft=400, hop_length=160) # 여기서 mfcc 파라미터에 n_fft나 stride 안 넣어주면, 모델 input이랑 달라지는 거임\n",
        "    #mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
        "    mfcc = scaler.fit_transform(mfcc)\n",
        "    padded_mfcc = pad2d(mfcc, 40)\n",
        "    padded_mfcc = np.expand_dims(padded_mfcc, [0, 3])\n",
        "\n",
        "    result = filler_classifier_model.predict(padded_mfcc)\n",
        "\n",
        "    # 판별 완료된 음성 파일 삭제\n",
        "    os.remove(\"temp.wav\")\n",
        "\n",
        "    return np.argmax(result)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPiJ-YidEjou"
      },
      "source": [
        "## 목소리 떨림 분석 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVN-uJs7EjUW"
      },
      "source": [
        "def cut_off(file):\n",
        "    snd = parselmouth.Sound(file)\n",
        "    pitch = snd.to_pitch()\n",
        "    pre_emphasized_snd = snd.copy()\n",
        "    pre_emphasized_snd.pre_emphasize()\n",
        "    spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
        "    data = pitch.selected_array['frequency']\n",
        "    q25, q75 = np.quantile(data, 0.25), np.quantile(data, 0.75) \n",
        "    iqr = q75 -q25\n",
        "    cut_off = iqr *0.5\n",
        "    lower, upper = q25 - cut_off, q75 + cut_off \n",
        "    return lower, upper\n",
        "\n",
        "def cut_mean(file):\n",
        "    snd = parselmouth.Sound(file)\n",
        "    pitch = snd.to_pitch()\n",
        "    pre_emphasized_snd = snd.copy()\n",
        "    pre_emphasized_snd.pre_emphasize()\n",
        "    spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
        "    data = pitch.selected_array['frequency']\n",
        "    m = np.mean(data)\n",
        "    upper = m + 400\n",
        "    lower = m - 100\n",
        "    return upper, lower\n",
        "\n",
        "def brr_ok(file,up,down,audio_len):\n",
        "    snd = parselmouth.Sound(file)\n",
        "    pitch = snd.to_pitch()\n",
        "\n",
        "    pre_emphasized_snd = snd.copy()\n",
        "    pre_emphasized_snd.pre_emphasize()\n",
        "    spectrogram = pre_emphasized_snd.to_spectrogram(window_length=0.03, maximum_frequency=8000)\n",
        "    max(pitch.selected_array['frequency'])\n",
        "    count = 0\n",
        "    brr = False\n",
        "    for i in pitch.selected_array['frequency']:\n",
        "        if i> up or i< down:\n",
        "            #brr = True\n",
        "            count = 1 + count\n",
        "    brr_sec = count/audio_len\n",
        "    if brr_sec > 3:\n",
        "        brr = True\n",
        "    return brr"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMuH6FJDrs6g"
      },
      "source": [
        "## 비침묵/침묵 구간 자르기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cft1wxXe1Z9n"
      },
      "source": [
        "def create_json(audio_file):\n",
        "    intervals_jsons = []\n",
        "\n",
        "    # min_silence_length = 70\n",
        "    min_silence_length = 70\n",
        "    intervals = detect_nonsilent(audio_file,\n",
        "                                min_silence_len=min_silence_length,\n",
        "                                silence_thresh=-32.64\n",
        "                                )\n",
        "  \n",
        "    if intervals[0][0] != 0:\n",
        "        intervals_jsons.append({'start':0,'end':intervals[0][0],'tag':'0000'}) # tag: 0000 means silence\n",
        "    \n",
        "    non_silence_start = intervals[0][0]\n",
        "    before_silence_start = intervals[0][1]\n",
        "\n",
        "    for interval in intervals:\n",
        "        interval_audio = audio_file[interval[0]:interval[1]]\n",
        "\n",
        "     # 800ms초 이상의 공백 부분 처리\n",
        "        if (interval[0]-before_silence_start) >= 800:\n",
        "            intervals_jsons.append({'start':non_silence_start,'end':before_silence_start+200,'tag':'1000'}) # tag: 1000 means non-slience\n",
        "            non_silence_start = interval[0]-200\n",
        "            intervals_jsons.append({'start':before_silence_start,'end':interval[0],'tag':'0000'}) # tag: 0000 means slience\n",
        "\n",
        "        if predict_filler(interval_audio) == 0 : # 추임새인 경우\n",
        "            if len(interval_audio) <= 460: # 비침묵 구간이 460ms보다 짧은 경우\n",
        "                intervals_jsons.append({'start':non_silence_start,'end':interval[0],'tag':'1000'}) # tag: 1000 means non-slience\n",
        "                non_silence_start = interval[0]\n",
        "                intervals_jsons.append({'start':interval[0],'end':interval[1],'tag':'1111'})\n",
        "            else: # 비침묵 구간이 460ms보다 긴 경우\n",
        "                non_silence_start = shorter_filler(intervals_jsons, interval_audio, min_silence_length, interval[0], non_silence_start)\n",
        "        \n",
        "        before_silence_start = interval[1]\n",
        "\n",
        "    if non_silence_start != len(audio_file):\n",
        "        intervals_jsons.append({'start':non_silence_start,'end':len(audio_file),'tag':'1000'})\n",
        "\n",
        "    return intervals_jsons"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DEHThTR6Ad1"
      },
      "source": [
        "def shorter_filler(json_result, audio_file, min_silence_len, start_time, non_silence_start):\n",
        "  \n",
        "    # 침묵 길이를 더 짧게\n",
        "    min_silence_length = (int)(min_silence_len/1.2)\n",
        "\n",
        "    intervals = detect_nonsilent(audio_file,\n",
        "                                min_silence_len=min_silence_length,\n",
        "                                silence_thresh=-32.64\n",
        "                                )\n",
        "    \n",
        "    for interval in intervals:\n",
        "        interval_audio = audio_file[interval[0]:interval[1]]\n",
        "\n",
        "        # padding 40 길이 이상인 경우 더 짧게\n",
        "        if (interval[1]-interval[0] >= 460):\n",
        "            non_silence_start = shorter_filler(json_result, interval_audio, min_silence_length, interval[0]+start_time, non_silence_start)\n",
        "\n",
        "        else: # padding 40 길이보다 짧은 경우 predict\n",
        "            if predict_filler(interval_audio) == 0 : # 추임새인 경우\n",
        "                json_result.append({'start':non_silence_start,'end':start_time+interval[0],'tag':'1000'}) # tag: 1000 means non-slience\n",
        "                non_silence_start = start_time + interval[0]\n",
        "        \n",
        "                # 추임새 tagging\n",
        "                json_result.append({'start':start_time+interval[0],'end':start_time+interval[1],'tag':'1111'}) # tag: 1111 means filler word\n",
        "        \n",
        "    return non_silence_start"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R859Oh_ar414"
      },
      "source": [
        "## 구간별 태깅(침묵/추임새) 및 STT(비침묵) 호출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2C9ReyTF99I"
      },
      "source": [
        "import json\n",
        "def STT_with_json_google(audio_file, jsons):\n",
        "    first_silence = 0\n",
        "    num = 0\n",
        "    unrecognizable_start = 0\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    transcript_json = []\n",
        "    statistics_filler_json = []\n",
        "    statistics_silence_json = []\n",
        "    filler_1 = 0  # '어' 횟수\n",
        "    filler_2 = 0  # '음' 횟수\n",
        "    filler_3 = 0  # '그' 횟수\n",
        "    audio_total_length = audio_file.duration_seconds\n",
        "    silence_interval = 0\n",
        "\n",
        "    for js in jsons:\n",
        "        if js['tag'] == '0000':\n",
        "\n",
        "            # 통역 개시 지연시간\n",
        "            if num == 0:\n",
        "                first_silence = first_silence + (js['end']-js['start'])/1000\n",
        "\n",
        "            else:\n",
        "                silence_interval = silence_interval + (js['end']-js['start'])/1000\n",
        "                silence = \"(침묵 \" + str(round((js['end']-js['start'])/1000)) + \"초)..\"\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'0000','result':silence})\n",
        "\n",
        "\n",
        "        elif js['tag'] == '1111':\n",
        "            # 통역 개시 지연시간\n",
        "            if num == 0:\n",
        "                silence = \"(침묵 \" + str(round(first_silence)) + \"초)..\"\n",
        "                transcript_json.append({'start':0,'end':js['start'],'tag':'0000','result':silence})\n",
        "                first_silence_interval = first_silence\n",
        "            # 추임새(어, 음, 그) 구분  \n",
        "            filler_type = predict_filler_type(audio_file[js['start']:js['end']])\n",
        "            if filler_type == 0 :\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1001','result':'어(추임새)'})\n",
        "                filler_1 = filler_1 + 1\n",
        "            elif filler_type == 1:\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1010','result':'음(추임새)'})\n",
        "                filler_2 = filler_2 + 1\n",
        "            else:\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1100','result':'그(추임새)'})\n",
        "                filler_3 = filler_3 + 1\n",
        "            num = num + 1\n",
        "   \n",
        "\n",
        "        elif js['tag'] == '1000': # 이때만 stt 사용. 이때만 발화구간\n",
        "\n",
        "            if unrecognizable_start != 0:\n",
        "                audio_file[unrecognizable_start:js['end']].export(\"temp.wav\", format=\"wav\")\n",
        "            else:\n",
        "                audio_file[js['start']:js['end']].export(\"temp.wav\", format=\"wav\")\n",
        "\n",
        "            temp_audio_file = sr.AudioFile('temp.wav')\n",
        "\n",
        "            with temp_audio_file as source:\n",
        "                audio = r.record(source)\n",
        "\n",
        "\n",
        "            try:\n",
        "                stt = r.recognize_google(audio_data=audio, language=\"ko-KR\")\n",
        "                brr_check = brr_ok(audio_thul,brrupper,brrlower,audio_len)\n",
        "                # 통역 개시 지연시간\n",
        "                if num == 0:\n",
        "                    silence = \"(침묵 \" + str(round(first_silence)) + \"초)..\"\n",
        "                    transcript_json.append({'start':0,'end':js['start'],'tag':'0000','result':silence})\n",
        "                    first_silence_interval = first_silence\n",
        "                if unrecognizable_start != 0:\n",
        "                    if brr_check == True:\n",
        "                        transcript_json.append({'start':unrecognizable_start,'end':js['end'],'tag':'1000','result':stt+\"(떨림)\"})\n",
        "                    else:\n",
        "                        transcript_json.append({'start':unrecognizable_start,'end':js['end'],'tag':'1000','result':stt})\n",
        "                    \n",
        "                else:\n",
        "                    #transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt})\n",
        "                    if brr_check == True:\n",
        "                        transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt+\"(떨림)\"})\n",
        "                    else:\n",
        "                        transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt})\n",
        "                unrecognizable_start = 0\n",
        "                num = num + 1\n",
        "            except:\n",
        "                if unrecognizable_start == 0:\n",
        "                    unrecognizable_start = js['start']\n",
        "\n",
        "\n",
        "    statistics_filler_json.append({'어':filler_1, '음':filler_2, '그':filler_3})\n",
        "    statistics_silence_json.append({'통역개시지연시간':100 * first_silence_interval/audio_total_length, '침묵시간':100 * silence_interval/audio_total_length, '발화시간':100 * (audio_total_length - first_silence - silence_interval)/audio_total_length})\n",
        "    return transcript_json, statistics_filler_json, statistics_silence_json"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLeW9KGPGy4C"
      },
      "source": [
        "import json\n",
        "def STT_with_json_google(audio_file, jsons,brrupper,brrlower):\n",
        "    first_silence = 0\n",
        "    num = 0\n",
        "    unrecognizable_start = 0\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    transcript_json = []\n",
        "    statistics_filler_json = []\n",
        "    statistics_silence_json = []\n",
        "    filler_1 = 0  # '어' 횟수\n",
        "    filler_2 = 0  # '음' 횟수\n",
        "    filler_3 = 0  # '그' 횟수\n",
        "    audio_total_length = audio_file.duration_seconds\n",
        "    silence_interval = 0\n",
        "# js 각각의 구간\n",
        "    for js in jsons: #구간 리스트 :\n",
        "        if js['tag'] == '0000':\n",
        "\n",
        "            # 통역 개시 지연시간\n",
        "            if num == 0:\n",
        "                first_silence = first_silence + (js['end']-js['start'])/1000\n",
        "\n",
        "            else:\n",
        "                silence_interval = silence_interval + (js['end']-js['start'])/1000\n",
        "                silence = \"(침묵 \" + str(round((js['end']-js['start'])/1000)) + \"초)..\"\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'0000','result':silence})\n",
        "\n",
        "        elif js['tag'] == '1111':\n",
        "            # 통역 개시 지연시간\n",
        "            if num == 0:\n",
        "                silence = \"(침묵 \" + str(round(first_silence)) + \"초)..\"\n",
        "                transcript_json.append({'start':0,'end':js['start'],'tag':'0000','result':silence})\n",
        "                first_silence_interval = first_silence\n",
        "            # 추임새(어, 음, 그) 구분  \n",
        "            filler_type = predict_filler_type(audio_file[js['start']:js['end']])\n",
        "            if filler_type == 0 :\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1001','result':'어(추임새)'})\n",
        "                filler_1 = filler_1 + 1\n",
        "            elif filler_type == 1:\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1010','result':'음(추임새)'})\n",
        "                filler_2 = filler_2 + 1\n",
        "            else:\n",
        "                transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1100','result':'그(추임새)'})\n",
        "                filler_3 = filler_3 + 1\n",
        "            num = num + 1\n",
        "   \n",
        "        elif js['tag'] == '1000': # 이때만 stt 사용. 이때만 발화구간\n",
        "\n",
        "            if unrecognizable_start != 0:\n",
        "                audio_file[unrecognizable_start:js['end']].export(\"temp.wav\", format=\"wav\")\n",
        "                audio_len = js['end'] - unrecognizable_start\n",
        "            else:\n",
        "                audio_file[js['start']:js['end']].export(\"temp.wav\", format=\"wav\")\n",
        "                audio_len = js['end'] - js['start']\n",
        "            audio_len =audio_len/1000\n",
        "            audio_thul = \"temp.wav\"\n",
        "            temp_audio_file = sr.AudioFile('temp.wav')\n",
        "\n",
        "            with temp_audio_file as source:\n",
        "                audio = r.record(source)\n",
        "\n",
        "\n",
        "            try:\n",
        "                stt = r.recognize_google(audio_data=audio, language=\"ko-KR\")\n",
        "                brr_check = brr_ok(audio_thul,brrupper,brrlower,audio_len)\n",
        "                # 통역 개시 지연시간\n",
        "                if num == 0:\n",
        "                    silence = \"(침묵 \" + str(round(first_silence)) + \"초)..\"\n",
        "                    transcript_json.append({'start':0,'end':js['start'],'tag':'0000','result':silence})\n",
        "                    first_silence_interval = first_silence\n",
        "                if unrecognizable_start != 0:\n",
        "                    if brr_check == True:\n",
        "                        transcript_json.append({'start':unrecognizable_start,'end':js['end'],'tag':'1000','result':stt+\"(떨림)\"})\n",
        "                    else:\n",
        "                        transcript_json.append({'start':unrecognizable_start,'end':js['end'],'tag':'1000','result':stt})\n",
        "                    \n",
        "                else:\n",
        "                    #transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt})\n",
        "                    if brr_check == True:\n",
        "                        transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt+\"(떨림)\"})\n",
        "                    else:\n",
        "                        transcript_json.append({'start':js['start'],'end':js['end'],'tag':'1000','result':stt})\n",
        "                unrecognizable_start = 0\n",
        "                num = num + 1\n",
        "            except:\n",
        "                if unrecognizable_start == 0:\n",
        "                    unrecognizable_start = js['start']\n",
        "\n",
        "    statistics_filler_json.append({'어':filler_1, '음':filler_2, '그':filler_3})\n",
        "    statistics_silence_json.append({'통역개시지연시간':100 * first_silence_interval/audio_total_length, '침묵시간':100 * silence_interval/audio_total_length, '발화시간':100 * (audio_total_length - first_silence - silence_interval)/audio_total_length})\n",
        "    return transcript_json, statistics_filler_json, statistics_silence_json\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dUN-QibsQXt"
      },
      "source": [
        "## 최종 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUMS3Mjr6bDh"
      },
      "source": [
        "def make_transcript_google(audio_file_path, brrupper, brrlower):\n",
        "    audio = AudioSegment.from_mp3(audio_file_path)\n",
        "    normalized_audio = match_target_amplitude(audio, -20.0)\n",
        "    intervals_jsons = create_json(normalized_audio)\n",
        "    transcript_json, statistics_filler_json, statistics_silence_json = STT_with_json_google(normalized_audio, intervals_jsons, brrupper, brrlower)\n",
        "\n",
        "    # return transcript_json\n",
        "    return transcript_json, statistics_filler_json, statistics_silence_json"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTuwNGohtPgg"
      },
      "source": [
        "import pandas as pd\n",
        "def json_to_text(transcript):\n",
        "    temp = pd.DataFrame(transcript)\n",
        "    # 추임새가 연속된 중복 행으로 나오면 첫 번째 행만 남기기\n",
        "    temp_ = temp.loc[temp['result'] != temp['result'].shift()].reset_index(drop=True)\n",
        "    \n",
        "    text = ''\n",
        "    for txt in temp_['result']:\n",
        "        text += txt + ' '\n",
        "    text = text.rstrip()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAud7sgeHy7g"
      },
      "source": [
        "# 떨림분석을 위한 코드\n",
        "lower,upper = cut_off(\"/content/drive/MyDrive/test/custom5.wav\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4et1XRztavm",
        "outputId": "772aa045-6e62-4e16-ae6d-720eab1fdcf5"
      },
      "source": [
        "transcript_json, statistics_filler_json, statistics_silence_json = make_transcript_google(\"/content/drive/MyDrive/test/custom5.wav\", upper, lower)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=160\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=272\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=48\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=48\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=160\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=272\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=336\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=96\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=48\n",
            "  n_fft, y.shape[-1]\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=400 is too small for input signal of length=48\n",
            "  n_fft, y.shape[-1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpOplNM1tdK6",
        "outputId": "ad263c85-4c7b-4235-bf2b-e9e8ec761db4"
      },
      "source": [
        "transcript_json\n",
        "# 0.4초\n",
        "# short_filler"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'end': 2221, 'result': '(침묵 2초)..', 'start': 0, 'tag': '0000'},\n",
              " {'end': 2227, 'result': '그(추임새)', 'start': 2221, 'tag': '1100'},\n",
              " {'end': 2234, 'result': '그(추임새)', 'start': 2228, 'tag': '1100'},\n",
              " {'end': 2245, 'result': '그(추임새)', 'start': 2235, 'tag': '1100'},\n",
              " {'end': 2252, 'result': '그(추임새)', 'start': 2246, 'tag': '1100'},\n",
              " {'end': 2259, 'result': '그(추임새)', 'start': 2253, 'tag': '1100'},\n",
              " {'end': 2277, 'result': '그(추임새)', 'start': 2260, 'tag': '1100'},\n",
              " {'end': 2534, 'result': '어(추임새)', 'start': 2278, 'tag': '1001'},\n",
              " {'end': 2556, 'result': '그(추임새)', 'start': 2535, 'tag': '1100'},\n",
              " {'end': 2578, 'result': '그(추임새)', 'start': 2557, 'tag': '1100'},\n",
              " {'end': 2600, 'result': '그(추임새)', 'start': 2579, 'tag': '1100'},\n",
              " {'end': 2676, 'result': '그(추임새)', 'start': 2670, 'tag': '1100'},\n",
              " {'end': 2683, 'result': '그(추임새)', 'start': 2677, 'tag': '1100'},\n",
              " {'end': 2687, 'result': '그(추임새)', 'start': 2684, 'tag': '1100'},\n",
              " {'end': 5511, 'result': '실제로 추임새 모델들을(떨림)', 'start': 2073, 'tag': '1000'},\n",
              " {'end': 6111, 'result': '(침묵 1초)..', 'start': 5311, 'tag': '0000'},\n",
              " {'end': 6921, 'result': '사용해서', 'start': 5911, 'tag': '1000'},\n",
              " {'end': 7711, 'result': '(침묵 1초)..', 'start': 6721, 'tag': '0000'},\n",
              " {'end': 8035, 'result': '음(추임새)', 'start': 7711, 'tag': '1010'},\n",
              " {'end': 11547, 'result': '그 전사 알고리즘 API를 돌려보니', 'start': 7511, 'tag': '1000'},\n",
              " {'end': 12333, 'result': '(침묵 1초)..', 'start': 11347, 'tag': '0000'},\n",
              " {'end': 12786, 'result': '음(추임새)', 'start': 12333, 'tag': '1010'},\n",
              " {'end': 12790, 'result': '그(추임새)', 'start': 12787, 'tag': '1100'},\n",
              " {'end': 13952, 'result': '별도로', 'start': 12133, 'tag': '1000'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bWbvIsLRtPaB",
        "outputId": "9122ae96-d472-4d01-ccf5-5d6702525fe6"
      },
      "source": [
        "json_to_text(transcript_json)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(침묵 2초).. 그(추임새) 어(추임새) 그(추임새) 실제로 추임새 모델들을(떨림) (침묵 1초).. 사용해서 (침묵 1초).. 음(추임새) 그 전사 알고리즘 API를 돌려보니 (침묵 1초).. 음(추임새) 그(추임새) 별도로'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqjrWpVGGOMa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm-pU41stwdg"
      },
      "source": [
        "## Reference\n",
        "https://github.com/EwhaSpeakUP/SpeakUP_ML <br>\n",
        "https://youdaeng-com.tistory.com/5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckuj8ZzFF67S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}